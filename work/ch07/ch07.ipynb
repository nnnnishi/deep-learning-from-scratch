{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 畳み込みニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 全体の構造\n",
    "1. 畳み込み層\n",
    "1. プーリング層\n",
    "1. Convolution/Pooling レイヤの実装\n",
    "1. CNNの実装\n",
    "1. CNNの可視化\n",
    "1. 代表的なCNN\n",
    "1. まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## im2col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n",
    "    filter_h : フィルターの高さ\n",
    "    filter_w : フィルターの幅\n",
    "    stride : ストライド\n",
    "    pad : パディング\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2次元配列\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : 入力データの形状（例：(10, 1, 28, 28)）\n",
    "    filter_h :\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutionレイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 中間データ（backward時に使用）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 重み・バイアスパラメータの勾配\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    \"\"\"単純なConvNet\n",
    "\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 入力サイズ（MNISTの場合は784）\n",
    "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
    "    output_size : 出力サイズ（MNISTの場合は10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
    "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
    "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"損失関数を求める\n",
    "        引数のxは入力データ、tは教師ラベル\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（数値微分）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNによるMNISTの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.300271110063349\n",
      "=== epoch:1, train acc:0.337, test acc:0.291 ===\n",
      "train loss:2.2975866275693506\n",
      "train loss:2.293404697892436\n",
      "train loss:2.2854476588362314\n",
      "train loss:2.2834112444368997\n",
      "train loss:2.2691710826988016\n",
      "train loss:2.248500918803198\n",
      "train loss:2.2412811837290336\n",
      "train loss:2.2170012265261674\n",
      "train loss:2.176509401217789\n",
      "train loss:2.1472430429428884\n",
      "train loss:2.138946619629743\n",
      "train loss:2.1040038533379906\n",
      "train loss:2.0400567087814996\n",
      "train loss:1.946243119207664\n",
      "train loss:1.925848155331851\n",
      "train loss:1.7797987553155543\n",
      "train loss:1.687114898801607\n",
      "train loss:1.6720712588918607\n",
      "train loss:1.5768461678810206\n",
      "train loss:1.5690736494170696\n",
      "train loss:1.4022421059491008\n",
      "train loss:1.4831429867278914\n",
      "train loss:1.2860845499663924\n",
      "train loss:1.2923836090202852\n",
      "train loss:1.1269657114375238\n",
      "train loss:1.158471343199259\n",
      "train loss:1.0324185411500635\n",
      "train loss:0.9286832679333815\n",
      "train loss:0.8000734379309911\n",
      "train loss:0.7421655642785646\n",
      "train loss:0.6951478975552974\n",
      "train loss:1.0108042939845308\n",
      "train loss:0.7245850319674096\n",
      "train loss:0.7765783351610596\n",
      "train loss:0.8892186582274104\n",
      "train loss:0.7331298221997616\n",
      "train loss:0.7340447691997393\n",
      "train loss:0.5648786990145199\n",
      "train loss:0.6221344576192105\n",
      "train loss:0.633064330045057\n",
      "train loss:0.5594271944806976\n",
      "train loss:0.4506910706165762\n",
      "train loss:0.6519397416953961\n",
      "train loss:0.4586375316745032\n",
      "train loss:0.5480274806465179\n",
      "train loss:0.6193831586171732\n",
      "train loss:0.715031693443169\n",
      "train loss:0.5646153848800908\n",
      "train loss:0.4332605158020402\n",
      "train loss:0.46529852284821366\n",
      "=== epoch:2, train acc:0.841, test acc:0.819 ===\n",
      "train loss:0.45589800660145513\n",
      "train loss:0.6050168933153253\n",
      "train loss:0.4235223601652048\n",
      "train loss:0.3648904054371566\n",
      "train loss:0.5813780540904965\n",
      "train loss:0.280321032653201\n",
      "train loss:0.6068446128478152\n",
      "train loss:0.4203183317596445\n",
      "train loss:0.46085207582849746\n",
      "train loss:0.4157586961102863\n",
      "train loss:0.6534306476796458\n",
      "train loss:0.3674435086558875\n",
      "train loss:0.3645018569892555\n",
      "train loss:0.4732111582917336\n",
      "train loss:0.46392637662266334\n",
      "train loss:0.20607550214801168\n",
      "train loss:0.36624166155540044\n",
      "train loss:0.3705719050951629\n",
      "train loss:0.31625365232182995\n",
      "train loss:0.3263568945505645\n",
      "train loss:0.5394134232651501\n",
      "train loss:0.36023634181604186\n",
      "train loss:0.36571978399454713\n",
      "train loss:0.316972211210928\n",
      "train loss:0.44392190310030066\n",
      "train loss:0.5672347919119624\n",
      "train loss:0.4575838480161842\n",
      "train loss:0.6235932187397065\n",
      "train loss:0.5408966389300032\n",
      "train loss:0.28654487552108376\n",
      "train loss:0.33855292576943646\n",
      "train loss:0.5197836926248451\n",
      "train loss:0.44106730025681434\n",
      "train loss:0.30764740623061226\n",
      "train loss:0.43163868483458456\n",
      "train loss:0.2437973434039086\n",
      "train loss:0.38861560194446054\n",
      "train loss:0.2995403605207615\n",
      "train loss:0.2963247897850468\n",
      "train loss:0.40059284336503914\n",
      "train loss:0.5158330487157923\n",
      "train loss:0.4316010260817071\n",
      "train loss:0.4611538076677906\n",
      "train loss:0.2690189052598956\n",
      "train loss:0.42143976780813963\n",
      "train loss:0.270477335766626\n",
      "train loss:0.25834685140278224\n",
      "train loss:0.40868130156238924\n",
      "train loss:0.3779474435535235\n",
      "train loss:0.38463953901272613\n",
      "=== epoch:3, train acc:0.887, test acc:0.87 ===\n",
      "train loss:0.35974691356439154\n",
      "train loss:0.4407424173232687\n",
      "train loss:0.31422098382568586\n",
      "train loss:0.3494754349370956\n",
      "train loss:0.3765531591068847\n",
      "train loss:0.24549056110508616\n",
      "train loss:0.28920779795558366\n",
      "train loss:0.4773623291634563\n",
      "train loss:0.21933554287849938\n",
      "train loss:0.23015997172648647\n",
      "train loss:0.2313410708317771\n",
      "train loss:0.20841894830653185\n",
      "train loss:0.21132462302073432\n",
      "train loss:0.2703032713056269\n",
      "train loss:0.32682054259634385\n",
      "train loss:0.24709866446957915\n",
      "train loss:0.29231708947671897\n",
      "train loss:0.2656892079792429\n",
      "train loss:0.4488626249605591\n",
      "train loss:0.21849512064138896\n",
      "train loss:0.319666378993946\n",
      "train loss:0.15360953409292577\n",
      "train loss:0.32037245063307884\n",
      "train loss:0.15760972935806394\n",
      "train loss:0.2117261182038432\n",
      "train loss:0.3498199200108698\n",
      "train loss:0.1778023716802871\n",
      "train loss:0.2884893712411132\n",
      "train loss:0.2702630878927001\n",
      "train loss:0.24806097968636698\n",
      "train loss:0.32550205952309225\n",
      "train loss:0.22151524114289145\n",
      "train loss:0.34558470600579255\n",
      "train loss:0.12684083534932622\n",
      "train loss:0.2952416839773971\n",
      "train loss:0.2251415691776106\n",
      "train loss:0.21495586846348572\n",
      "train loss:0.4045372771015153\n",
      "train loss:0.34705555359045603\n",
      "train loss:0.19713806209816837\n",
      "train loss:0.2288653340399509\n",
      "train loss:0.2953728094278106\n",
      "train loss:0.14316889634154129\n",
      "train loss:0.34927369467167707\n",
      "train loss:0.3391650286713544\n",
      "train loss:0.4644565250986537\n",
      "train loss:0.34488706239912803\n",
      "train loss:0.18118748033766427\n",
      "train loss:0.20111239613007384\n",
      "train loss:0.1969486923469294\n",
      "=== epoch:4, train acc:0.901, test acc:0.904 ===\n",
      "train loss:0.29725851904573575\n",
      "train loss:0.22960427519439752\n",
      "train loss:0.11307216261793494\n",
      "train loss:0.23995817871547243\n",
      "train loss:0.24202960453048988\n",
      "train loss:0.21827208561748296\n",
      "train loss:0.19244745595289264\n",
      "train loss:0.3217388432962938\n",
      "train loss:0.2674086803809561\n",
      "train loss:0.27045059225984697\n",
      "train loss:0.30353918524154866\n",
      "train loss:0.16652890182002253\n",
      "train loss:0.2767770640909759\n",
      "train loss:0.2124534328818\n",
      "train loss:0.29767174174416633\n",
      "train loss:0.15226530372583738\n",
      "train loss:0.22575252765878595\n",
      "train loss:0.22097989672719187\n",
      "train loss:0.12446613030050052\n",
      "train loss:0.2579315729732705\n",
      "train loss:0.17936542774108788\n",
      "train loss:0.22199982834211818\n",
      "train loss:0.30814427725966237\n",
      "train loss:0.21994137507482242\n",
      "train loss:0.19469227254166593\n",
      "train loss:0.22849584102894327\n",
      "train loss:0.2159591682808161\n",
      "train loss:0.2629267161772764\n",
      "train loss:0.18575829889082307\n",
      "train loss:0.2291759174472129\n",
      "train loss:0.10115016066417341\n",
      "train loss:0.12436947050768055\n",
      "train loss:0.20993352054442507\n",
      "train loss:0.25253179435572687\n",
      "train loss:0.22442503856294227\n",
      "train loss:0.2538982759806753\n",
      "train loss:0.12385785426202676\n",
      "train loss:0.2497163675836599\n",
      "train loss:0.17933932350402737\n",
      "train loss:0.15202284096944876\n",
      "train loss:0.28894917928790287\n",
      "train loss:0.32835484638609386\n",
      "train loss:0.2253474197092008\n",
      "train loss:0.1993612526372115\n",
      "train loss:0.19172742028963\n",
      "train loss:0.19798356509504703\n",
      "train loss:0.14523213225763257\n",
      "train loss:0.1792397281799892\n",
      "train loss:0.195612905731068\n",
      "train loss:0.23315136774654202\n",
      "=== epoch:5, train acc:0.916, test acc:0.92 ===\n",
      "train loss:0.24822711376654247\n",
      "train loss:0.11413927174472578\n",
      "train loss:0.20033204388805909\n",
      "train loss:0.1464282807915654\n",
      "train loss:0.12043696627624699\n",
      "train loss:0.16722040855167272\n",
      "train loss:0.14812484186594166\n",
      "train loss:0.26861594918259896\n",
      "train loss:0.15446169495354087\n",
      "train loss:0.12978746534609906\n",
      "train loss:0.30317080935833574\n",
      "train loss:0.17299002168772493\n",
      "train loss:0.2391498981669506\n",
      "train loss:0.20336451276294723\n",
      "train loss:0.1478845663479633\n",
      "train loss:0.15119053079116615\n",
      "train loss:0.3580364159949244\n",
      "train loss:0.16929034171349489\n",
      "train loss:0.16288628271832473\n",
      "train loss:0.17303392903862636\n",
      "train loss:0.18131618592400411\n",
      "train loss:0.17306099683640813\n",
      "train loss:0.18054905477031166\n",
      "train loss:0.20468411683473264\n",
      "train loss:0.14532306110129636\n",
      "train loss:0.2639638135722553\n",
      "train loss:0.12418509044118525\n",
      "train loss:0.22802451022897008\n",
      "train loss:0.33505605032010577\n",
      "train loss:0.19752517816139156\n",
      "train loss:0.2915276827468216\n",
      "train loss:0.13632901891267044\n",
      "train loss:0.18111395686929857\n",
      "train loss:0.4769581414096058\n",
      "train loss:0.10593719769499085\n",
      "train loss:0.1619595947315738\n",
      "train loss:0.39230311684128016\n",
      "train loss:0.17062446560546626\n",
      "train loss:0.19152423685664993\n",
      "train loss:0.22888788274726124\n",
      "train loss:0.15768990893253793\n",
      "train loss:0.17658552345420042\n",
      "train loss:0.15165010476065438\n",
      "train loss:0.2261893760977262\n",
      "train loss:0.21848729204922115\n",
      "train loss:0.17667125582669707\n",
      "train loss:0.20998735791959067\n",
      "train loss:0.14393108904066343\n",
      "train loss:0.20192067532684332\n",
      "train loss:0.14369053997228035\n",
      "=== epoch:6, train acc:0.929, test acc:0.927 ===\n",
      "train loss:0.19388848054314042\n",
      "train loss:0.14703524995321124\n",
      "train loss:0.1867784396865205\n",
      "train loss:0.18571343818468036\n",
      "train loss:0.12488911490816404\n",
      "train loss:0.1299465486617874\n",
      "train loss:0.12904686812330246\n",
      "train loss:0.13144173302901804\n",
      "train loss:0.13229824983827426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.27800772793801914\n",
      "train loss:0.11863698275767563\n",
      "train loss:0.1059110600569051\n",
      "train loss:0.12888785160350627\n",
      "train loss:0.2022705058395756\n",
      "train loss:0.2273278013637858\n",
      "train loss:0.2914659803399641\n",
      "train loss:0.09215401050386297\n",
      "train loss:0.08605019947445464\n",
      "train loss:0.12656852558027484\n",
      "train loss:0.16740321446652714\n",
      "train loss:0.14369581743844898\n",
      "train loss:0.23715508650447908\n",
      "train loss:0.14599379600223739\n",
      "train loss:0.12026792360260685\n",
      "train loss:0.19172827203415513\n",
      "train loss:0.12322414000898826\n",
      "train loss:0.2521471658406821\n",
      "train loss:0.09794749450666847\n",
      "train loss:0.11526369733837051\n",
      "train loss:0.1479995636716483\n",
      "train loss:0.14354735270092447\n",
      "train loss:0.16979128950189376\n",
      "train loss:0.1405231534042595\n",
      "train loss:0.17796771868452382\n",
      "train loss:0.11531173638901883\n",
      "train loss:0.2681563143740129\n",
      "train loss:0.10348813747393627\n",
      "train loss:0.16593084109708278\n",
      "train loss:0.13342893876935313\n",
      "train loss:0.1288722227443149\n",
      "train loss:0.11120774752759856\n",
      "train loss:0.12322408000591739\n",
      "train loss:0.18042336260604044\n",
      "train loss:0.09411759065705891\n",
      "train loss:0.10760666119534568\n",
      "train loss:0.14113125255243353\n",
      "train loss:0.14617661533783866\n",
      "train loss:0.05475854436385206\n",
      "train loss:0.15928893905768526\n",
      "train loss:0.2010596622464762\n",
      "=== epoch:7, train acc:0.951, test acc:0.921 ===\n",
      "train loss:0.08301533125554045\n",
      "train loss:0.059626677779329416\n",
      "train loss:0.12187544950554141\n",
      "train loss:0.09104009706128893\n",
      "train loss:0.175264093645911\n",
      "train loss:0.07129716248321205\n",
      "train loss:0.1029738639536417\n",
      "train loss:0.05976762549218271\n",
      "train loss:0.14021498889086859\n",
      "train loss:0.07425971157275107\n",
      "train loss:0.12616450191799417\n",
      "train loss:0.1830709683310594\n",
      "train loss:0.06122635683491092\n",
      "train loss:0.23179981350175982\n",
      "train loss:0.12855229090021703\n",
      "train loss:0.31723377911752526\n",
      "train loss:0.10960493936851695\n",
      "train loss:0.12195452373642088\n",
      "train loss:0.1290297366289561\n",
      "train loss:0.13907315189959127\n",
      "train loss:0.2504693285666141\n",
      "train loss:0.12147207647367544\n",
      "train loss:0.13361112211607357\n",
      "train loss:0.1973506728060982\n",
      "train loss:0.21649897647735372\n",
      "train loss:0.12501768000175986\n",
      "train loss:0.06213451879203195\n",
      "train loss:0.15008878376941925\n",
      "train loss:0.15761104276699586\n",
      "train loss:0.1360144483991445\n",
      "train loss:0.10496423200020058\n",
      "train loss:0.10829520723822596\n",
      "train loss:0.12280327324636583\n",
      "train loss:0.16105209077218097\n",
      "train loss:0.13526845825856235\n",
      "train loss:0.06867873244553938\n",
      "train loss:0.16958822358087616\n",
      "train loss:0.08529887748767218\n",
      "train loss:0.10424916361975382\n",
      "train loss:0.24352197912028892\n",
      "train loss:0.13537935014760272\n",
      "train loss:0.1243041958173227\n",
      "train loss:0.12749252388368046\n",
      "train loss:0.16686755715449159\n",
      "train loss:0.0718904685204466\n",
      "train loss:0.09652556135310653\n",
      "train loss:0.11856829408426325\n",
      "train loss:0.20839594752238166\n",
      "train loss:0.18948572238805497\n",
      "train loss:0.11552327118356244\n",
      "=== epoch:8, train acc:0.947, test acc:0.937 ===\n",
      "train loss:0.0674807172341192\n",
      "train loss:0.11408196850075669\n",
      "train loss:0.1336707750352749\n",
      "train loss:0.10340153025001289\n",
      "train loss:0.14875729872236612\n",
      "train loss:0.09568012841753248\n",
      "train loss:0.14145411974552058\n",
      "train loss:0.13434670952168068\n",
      "train loss:0.15138673266320463\n",
      "train loss:0.11008089070684937\n",
      "train loss:0.08713849030449607\n",
      "train loss:0.16473317351462582\n",
      "train loss:0.05417078418687959\n",
      "train loss:0.05960968670248151\n",
      "train loss:0.16022649729581084\n",
      "train loss:0.17125203335024586\n",
      "train loss:0.07796726894268734\n",
      "train loss:0.08818048572693733\n",
      "train loss:0.12789803664936333\n",
      "train loss:0.1689976023198639\n",
      "train loss:0.0859526711379149\n",
      "train loss:0.05273250908600218\n",
      "train loss:0.07691234839408628\n",
      "train loss:0.07893972634617885\n",
      "train loss:0.11038552883687482\n",
      "train loss:0.0699754367154036\n",
      "train loss:0.233782535284431\n",
      "train loss:0.20709001939002789\n",
      "train loss:0.12898151903494356\n",
      "train loss:0.05750419572768054\n",
      "train loss:0.11670044546190797\n",
      "train loss:0.07322199462281796\n",
      "train loss:0.1376878506279991\n",
      "train loss:0.08732489219232695\n",
      "train loss:0.05765717755217575\n",
      "train loss:0.09587622557980996\n",
      "train loss:0.19395733046471034\n",
      "train loss:0.0932747303808387\n",
      "train loss:0.06855286527770293\n",
      "train loss:0.10553204123267042\n",
      "train loss:0.13927184130212958\n",
      "train loss:0.11204795336968924\n",
      "train loss:0.07976094408872847\n",
      "train loss:0.11368078740153577\n",
      "train loss:0.12202514112099924\n",
      "train loss:0.14529529979058442\n",
      "train loss:0.1322031551182985\n",
      "train loss:0.08071139696723295\n",
      "train loss:0.047246623957430886\n",
      "train loss:0.058570287866523844\n",
      "=== epoch:9, train acc:0.958, test acc:0.942 ===\n",
      "train loss:0.1159832078484664\n",
      "train loss:0.03303865806961209\n",
      "train loss:0.14254810714866134\n",
      "train loss:0.040995902869892624\n",
      "train loss:0.08999807479377615\n",
      "train loss:0.07432217827343603\n",
      "train loss:0.32647295477173033\n",
      "train loss:0.08802342043342647\n",
      "train loss:0.17443951691312673\n",
      "train loss:0.14582139866873509\n",
      "train loss:0.10651205801945077\n",
      "train loss:0.19331557462680465\n",
      "train loss:0.11805881277082658\n",
      "train loss:0.056123019630687054\n",
      "train loss:0.06863372919374348\n",
      "train loss:0.06936607999136109\n",
      "train loss:0.1498319945649466\n",
      "train loss:0.0600599056231543\n",
      "train loss:0.06704910147672889\n",
      "train loss:0.1791002592932226\n",
      "train loss:0.06240122408412759\n",
      "train loss:0.05177146968769482\n",
      "train loss:0.11624172286949973\n",
      "train loss:0.1908670663944881\n",
      "train loss:0.08091379041666119\n",
      "train loss:0.14526051430725637\n",
      "train loss:0.09982162217914485\n",
      "train loss:0.07206507009754294\n",
      "train loss:0.07321401441833399\n",
      "train loss:0.09060113280072737\n",
      "train loss:0.12305336888559798\n",
      "train loss:0.05408785207846734\n",
      "train loss:0.07976059007655902\n",
      "train loss:0.07415821759192057\n",
      "train loss:0.032931722741302696\n",
      "train loss:0.0726968094272722\n",
      "train loss:0.21182779268239205\n",
      "train loss:0.060709342488514985\n",
      "train loss:0.08763477162008254\n",
      "train loss:0.08950798642770719\n",
      "train loss:0.05903109663923026\n",
      "train loss:0.032531418186320896\n",
      "train loss:0.08308349280116387\n",
      "train loss:0.06405551406908207\n",
      "train loss:0.11720391505601968\n",
      "train loss:0.1858681265698558\n",
      "train loss:0.10801891006208418\n",
      "train loss:0.19921125543034776\n",
      "train loss:0.06781323675966221\n",
      "train loss:0.07036880496644397\n",
      "=== epoch:10, train acc:0.966, test acc:0.955 ===\n",
      "train loss:0.11826063594496794\n",
      "train loss:0.0902207094755049\n",
      "train loss:0.08116530531981325\n",
      "train loss:0.06921661220025739\n",
      "train loss:0.05189339953169453\n",
      "train loss:0.03550606330009152\n",
      "train loss:0.08301754555516602\n",
      "train loss:0.05783983632719094\n",
      "train loss:0.08144708699998189\n",
      "train loss:0.06615652582533717\n",
      "train loss:0.10255161466446681\n",
      "train loss:0.06289010546781415\n",
      "train loss:0.04551381682158492\n",
      "train loss:0.04794655405821639\n",
      "train loss:0.10268808302815653\n",
      "train loss:0.08654184487425859\n",
      "train loss:0.07039833247758814\n",
      "train loss:0.07182778349731639\n",
      "train loss:0.0668526042445533\n",
      "train loss:0.08016052014881611\n",
      "train loss:0.07852659466793598\n",
      "train loss:0.13043236952664666\n",
      "train loss:0.07319087408548468\n",
      "train loss:0.046585005221884045\n",
      "train loss:0.06449695010536854\n",
      "train loss:0.0871743519599613\n",
      "train loss:0.09162380237999752\n",
      "train loss:0.06149746523174197\n",
      "train loss:0.04498918784457498\n",
      "train loss:0.11657858281271792\n",
      "train loss:0.0992758147280186\n",
      "train loss:0.06642365672900674\n",
      "train loss:0.1248162894268861\n",
      "train loss:0.09397987139968236\n",
      "train loss:0.14781533091322321\n",
      "train loss:0.11823810251435406\n",
      "train loss:0.06910374719501308\n",
      "train loss:0.06498402536946946\n",
      "train loss:0.11929803185235055\n",
      "train loss:0.09291322809712338\n",
      "train loss:0.046496327947684574\n",
      "train loss:0.14142948309668602\n",
      "train loss:0.07225580059723363\n",
      "train loss:0.16194152595293804\n",
      "train loss:0.06952067032552954\n",
      "train loss:0.1241534664610105\n",
      "train loss:0.1056247079330804\n",
      "train loss:0.07263737788038559\n",
      "train loss:0.06581756628565244\n",
      "train loss:0.08136697745215703\n",
      "=== epoch:11, train acc:0.972, test acc:0.949 ===\n",
      "train loss:0.04993739195320391\n",
      "train loss:0.09120163301742215\n",
      "train loss:0.031647798435538416\n",
      "train loss:0.07482241746110113\n",
      "train loss:0.07904409980542247\n",
      "train loss:0.0472959328565492\n",
      "train loss:0.18116359882056698\n",
      "train loss:0.01505035650112657\n",
      "train loss:0.03760217488147824\n",
      "train loss:0.11303026586375803\n",
      "train loss:0.08672946012154643\n",
      "train loss:0.15119219668486655\n",
      "train loss:0.059285556873355176\n",
      "train loss:0.024814452685151114\n",
      "train loss:0.06126941706740096\n",
      "train loss:0.022150187252905993\n",
      "train loss:0.044631543570849905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0635153092907443\n",
      "train loss:0.021789950234342128\n",
      "train loss:0.04636462696334475\n",
      "train loss:0.03996320867847247\n",
      "train loss:0.09239882770000075\n",
      "train loss:0.06079027917159152\n",
      "train loss:0.03276994633111809\n",
      "train loss:0.04803739181818508\n",
      "train loss:0.161051735625655\n",
      "train loss:0.07400546039492566\n",
      "train loss:0.07942062444920053\n",
      "train loss:0.06318179728597556\n",
      "train loss:0.03954765597326246\n",
      "train loss:0.1494811168248532\n",
      "train loss:0.12089931235219557\n",
      "train loss:0.05545569341841592\n",
      "train loss:0.10216729201943897\n",
      "train loss:0.07179479537191871\n",
      "train loss:0.11258959047492098\n",
      "train loss:0.029048529092404553\n",
      "train loss:0.06223096839033421\n",
      "train loss:0.1057464068055622\n",
      "train loss:0.035667931277889024\n",
      "train loss:0.03587634477532195\n",
      "train loss:0.11730177248970328\n",
      "train loss:0.03981806123237423\n",
      "train loss:0.05288680753363096\n",
      "train loss:0.03593872744279998\n",
      "train loss:0.0645309003651965\n",
      "train loss:0.07393458470016967\n",
      "train loss:0.0631663556116719\n",
      "train loss:0.08216115835999105\n",
      "train loss:0.04789784255650603\n",
      "=== epoch:12, train acc:0.976, test acc:0.957 ===\n",
      "train loss:0.13659526061667449\n",
      "train loss:0.10927406522695719\n",
      "train loss:0.10054765687118941\n",
      "train loss:0.03839536683388018\n",
      "train loss:0.052993150015677844\n",
      "train loss:0.038812099725010756\n",
      "train loss:0.06315632095209628\n",
      "train loss:0.09167661723381267\n",
      "train loss:0.06218977428011353\n",
      "train loss:0.0787460463902208\n",
      "train loss:0.06378327961316972\n",
      "train loss:0.11196457494590514\n",
      "train loss:0.05476526960695144\n",
      "train loss:0.07344747537868515\n",
      "train loss:0.05255099401808378\n",
      "train loss:0.09600197256626825\n",
      "train loss:0.0284099753419549\n",
      "train loss:0.11066156934145564\n",
      "train loss:0.04557758097156645\n",
      "train loss:0.1682510732324356\n",
      "train loss:0.06560983615701368\n",
      "train loss:0.09548017740619746\n",
      "train loss:0.09681673535044856\n",
      "train loss:0.08333547784566812\n",
      "train loss:0.06567111192838763\n",
      "train loss:0.023837772525761504\n",
      "train loss:0.09440467456619786\n",
      "train loss:0.09446934985630241\n",
      "train loss:0.06293557883040367\n",
      "train loss:0.030379257005797137\n",
      "train loss:0.09059128960342935\n",
      "train loss:0.05237046389920353\n",
      "train loss:0.03597547061339302\n",
      "train loss:0.0975016333670941\n",
      "train loss:0.048863155625461896\n",
      "train loss:0.01415468388377616\n",
      "train loss:0.03748573054851277\n",
      "train loss:0.03277830792871911\n",
      "train loss:0.1412760478799179\n",
      "train loss:0.04755971119497726\n",
      "train loss:0.0908980453708251\n",
      "train loss:0.020013940998339096\n",
      "train loss:0.04079474228372143\n",
      "train loss:0.05929855936122732\n",
      "train loss:0.051657685179353985\n",
      "train loss:0.030245772001775965\n",
      "train loss:0.05548163086763187\n",
      "train loss:0.04978419806272085\n",
      "train loss:0.03158947305706232\n",
      "train loss:0.053074221315193126\n",
      "=== epoch:13, train acc:0.979, test acc:0.95 ===\n",
      "train loss:0.11434477545545615\n",
      "train loss:0.10843052760658982\n",
      "train loss:0.038965084769289136\n",
      "train loss:0.0230187679184724\n",
      "train loss:0.054687537313792216\n",
      "train loss:0.09006423683137413\n",
      "train loss:0.11627279808554127\n",
      "train loss:0.08400495584682707\n",
      "train loss:0.0773114353604952\n",
      "train loss:0.061385407777314376\n",
      "train loss:0.058560717134661336\n",
      "train loss:0.031349077693910196\n",
      "train loss:0.0601142657203083\n",
      "train loss:0.04412283730266513\n",
      "train loss:0.054617083993171646\n",
      "train loss:0.03236050207128254\n",
      "train loss:0.06444933471733172\n",
      "train loss:0.01627742137366236\n",
      "train loss:0.01768706474984802\n",
      "train loss:0.03704274811512541\n",
      "train loss:0.053322950186317254\n",
      "train loss:0.03620918495959288\n",
      "train loss:0.08966471930327426\n",
      "train loss:0.0641121036845007\n",
      "train loss:0.04372628640386001\n",
      "train loss:0.0658059076130458\n",
      "train loss:0.03334467755456948\n",
      "train loss:0.032522031253265166\n",
      "train loss:0.09713898147894476\n",
      "train loss:0.04224754935653351\n",
      "train loss:0.07481262823415602\n",
      "train loss:0.08368133957284449\n",
      "train loss:0.06698190506242199\n",
      "train loss:0.031069113914906462\n",
      "train loss:0.05098295675343333\n",
      "train loss:0.05927304139405115\n",
      "train loss:0.05618762130383483\n",
      "train loss:0.1432904818186753\n",
      "train loss:0.18195642676016402\n",
      "train loss:0.030965097523958415\n",
      "train loss:0.06743751160100331\n",
      "train loss:0.04192537341433503\n",
      "train loss:0.030261582999284303\n",
      "train loss:0.046113972562738005\n",
      "train loss:0.038495052751721964\n",
      "train loss:0.04527857517995165\n",
      "train loss:0.05609588073466745\n",
      "train loss:0.043983213804131456\n",
      "train loss:0.0318325076787289\n",
      "train loss:0.02427252746698454\n",
      "=== epoch:14, train acc:0.98, test acc:0.954 ===\n",
      "train loss:0.11011498839923785\n",
      "train loss:0.07349958941249221\n",
      "train loss:0.07376460418779734\n",
      "train loss:0.0604178543044295\n",
      "train loss:0.038988376898909015\n",
      "train loss:0.05561863239447229\n",
      "train loss:0.05136427716958638\n",
      "train loss:0.01839100328118628\n",
      "train loss:0.026360233808041377\n",
      "train loss:0.03429637523647234\n",
      "train loss:0.09897483755593006\n",
      "train loss:0.07402231909654944\n",
      "train loss:0.04655194573613122\n",
      "train loss:0.05485831857269976\n",
      "train loss:0.07386684166735048\n",
      "train loss:0.042729999077727586\n",
      "train loss:0.09967942955660075\n",
      "train loss:0.061388232197157125\n",
      "train loss:0.041987334863149026\n",
      "train loss:0.023878970561139554\n",
      "train loss:0.030414106044602807\n",
      "train loss:0.018948679412759028\n",
      "train loss:0.03541815874138767\n",
      "train loss:0.0636040467520247\n",
      "train loss:0.09322876169843809\n",
      "train loss:0.07174122159045665\n",
      "train loss:0.020191270331138794\n",
      "train loss:0.04095111750263057\n",
      "train loss:0.034306549233760156\n",
      "train loss:0.0402271048188801\n",
      "train loss:0.04332402086841184\n",
      "train loss:0.016121221620158076\n",
      "train loss:0.06411068718713246\n",
      "train loss:0.040967551486451495\n",
      "train loss:0.01868979732947331\n",
      "train loss:0.04249089483737345\n",
      "train loss:0.09090095702931303\n",
      "train loss:0.0361569766864612\n",
      "train loss:0.02835798789058167\n",
      "train loss:0.05404816946660406\n",
      "train loss:0.052279035922009304\n",
      "train loss:0.027224178072247085\n",
      "train loss:0.019198131530527657\n",
      "train loss:0.04362549015586804\n",
      "train loss:0.07633512223959454\n",
      "train loss:0.05925955544127328\n",
      "train loss:0.02919344018178283\n",
      "train loss:0.034162457671557935\n",
      "train loss:0.03451173449247639\n",
      "train loss:0.020899315122015004\n",
      "=== epoch:15, train acc:0.985, test acc:0.958 ===\n",
      "train loss:0.015092197366680236\n",
      "train loss:0.03089967976397789\n",
      "train loss:0.04737927323599753\n",
      "train loss:0.06121632838318129\n",
      "train loss:0.02538840495853055\n",
      "train loss:0.03454089690831613\n",
      "train loss:0.07631455734353743\n",
      "train loss:0.0667178117229245\n",
      "train loss:0.0370898839647883\n",
      "train loss:0.01948096559507599\n",
      "train loss:0.02993948434614266\n",
      "train loss:0.04640998272787916\n",
      "train loss:0.0466026098407225\n",
      "train loss:0.0447894061288602\n",
      "train loss:0.01793567260232777\n",
      "train loss:0.09560880861610588\n",
      "train loss:0.07795409005505281\n",
      "train loss:0.019213259670046407\n",
      "train loss:0.029250265545561777\n",
      "train loss:0.02441055644248948\n",
      "train loss:0.049832910925328974\n",
      "train loss:0.024974625537295115\n",
      "train loss:0.06947405697562026\n",
      "train loss:0.018132480868505416\n",
      "train loss:0.014768663235519317\n",
      "train loss:0.018682988866591303\n",
      "train loss:0.040076318934808695\n",
      "train loss:0.04092313686957549\n",
      "train loss:0.045168173021389625\n",
      "train loss:0.0239411523014712\n",
      "train loss:0.023401018135748006\n",
      "train loss:0.060534626840608424\n",
      "train loss:0.03643985136776237\n",
      "train loss:0.03990259813518919\n",
      "train loss:0.040245026578911706\n",
      "train loss:0.040260836131545685\n",
      "train loss:0.08212726313836831\n",
      "train loss:0.03968760598990457\n",
      "train loss:0.03537139057966777\n",
      "train loss:0.05459267516536195\n",
      "train loss:0.040238822557751665\n",
      "train loss:0.06726080802147283\n",
      "train loss:0.0271253215594757\n",
      "train loss:0.07491509862532714\n",
      "train loss:0.04328420110549629\n",
      "train loss:0.05726129792512806\n",
      "train loss:0.01585264735192489\n",
      "train loss:0.04261678551604026\n",
      "train loss:0.0589225493152991\n",
      "train loss:0.01906021147769779\n",
      "=== epoch:16, train acc:0.987, test acc:0.961 ===\n",
      "train loss:0.043914448788142525\n",
      "train loss:0.03925545288983432\n",
      "train loss:0.038089108363921\n",
      "train loss:0.0620199016675073\n",
      "train loss:0.03660995143757504\n",
      "train loss:0.03889462769971273\n",
      "train loss:0.02830696604591248\n",
      "train loss:0.027915086464322517\n",
      "train loss:0.034430625107390186\n",
      "train loss:0.050111357711636446\n",
      "train loss:0.01023262564996641\n",
      "train loss:0.0313763941059901\n",
      "train loss:0.04950862052788648\n",
      "train loss:0.07461142760137938\n",
      "train loss:0.08386777649095987\n",
      "train loss:0.061311581453980565\n",
      "train loss:0.056754228674515364\n",
      "train loss:0.01261435302105549\n",
      "train loss:0.07099456015957842\n",
      "train loss:0.04643903519006834\n",
      "train loss:0.03814729204774418\n",
      "train loss:0.03944554509271005\n",
      "train loss:0.036003043788270093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.049707143844041936\n",
      "train loss:0.02974870277162429\n",
      "train loss:0.03331267791572533\n",
      "train loss:0.016517910869890143\n",
      "train loss:0.08864819476929651\n",
      "train loss:0.056816158499902424\n",
      "train loss:0.05474398539109709\n",
      "train loss:0.03726604201619287\n",
      "train loss:0.07508532701311835\n",
      "train loss:0.021182182734805682\n",
      "train loss:0.05102054431321225\n",
      "train loss:0.052639165285492985\n",
      "train loss:0.028753098178927503\n",
      "train loss:0.04020178250927495\n",
      "train loss:0.02361784854313713\n",
      "train loss:0.07911233349432896\n",
      "train loss:0.04119801897753492\n",
      "train loss:0.04475381510387865\n",
      "train loss:0.03889766876124216\n",
      "train loss:0.028215123902732238\n",
      "train loss:0.029187816387855153\n",
      "train loss:0.06518521803997741\n",
      "train loss:0.043507876228020395\n",
      "train loss:0.02666287990499924\n",
      "train loss:0.045914686805139146\n",
      "train loss:0.038827931749710755\n",
      "train loss:0.01867142064775914\n",
      "=== epoch:17, train acc:0.991, test acc:0.955 ===\n",
      "train loss:0.05988277222342577\n",
      "train loss:0.04805337274257285\n",
      "train loss:0.02202248343070209\n",
      "train loss:0.041069048132261525\n",
      "train loss:0.021670037555130674\n",
      "train loss:0.03287179503255461\n",
      "train loss:0.03638897981648951\n",
      "train loss:0.041303613762759095\n",
      "train loss:0.017401937420910285\n",
      "train loss:0.041855915635193724\n",
      "train loss:0.024477131713690144\n",
      "train loss:0.025034418103357145\n",
      "train loss:0.030185826271173485\n",
      "train loss:0.02332835811648242\n",
      "train loss:0.042956988143855686\n",
      "train loss:0.025407269672404083\n",
      "train loss:0.018386516408879602\n",
      "train loss:0.04834008024033066\n",
      "train loss:0.0342490523408562\n",
      "train loss:0.020773053219391612\n",
      "train loss:0.03942746639493662\n",
      "train loss:0.02729748902106391\n",
      "train loss:0.02843386583207181\n",
      "train loss:0.014006297535009935\n",
      "train loss:0.0225596507288376\n",
      "train loss:0.017973085317406056\n",
      "train loss:0.02494232229876062\n",
      "train loss:0.018870551117982307\n",
      "train loss:0.014243886673045906\n",
      "train loss:0.05124274315764295\n",
      "train loss:0.01051662080414653\n",
      "train loss:0.04942187115861252\n",
      "train loss:0.024508275078143783\n",
      "train loss:0.015356683864465915\n",
      "train loss:0.03301399598792585\n",
      "train loss:0.021304851846888035\n",
      "train loss:0.04367669379967158\n",
      "train loss:0.033482849161048794\n",
      "train loss:0.02362586827567955\n",
      "train loss:0.0221515475436309\n",
      "train loss:0.021164883168093104\n",
      "train loss:0.036880321664403956\n",
      "train loss:0.05375283583393597\n",
      "train loss:0.03459628260217858\n",
      "train loss:0.007861552017416567\n",
      "train loss:0.022041056769249823\n",
      "train loss:0.03599685407213701\n",
      "train loss:0.018643368740967524\n",
      "train loss:0.022381066154710763\n",
      "train loss:0.016314948769039585\n",
      "=== epoch:18, train acc:0.983, test acc:0.954 ===\n",
      "train loss:0.02347226473127057\n",
      "train loss:0.010353815329176593\n",
      "train loss:0.020323855562856954\n",
      "train loss:0.01539775417140774\n",
      "train loss:0.0122085744145949\n",
      "train loss:0.03572125796107482\n",
      "train loss:0.014207263021311858\n",
      "train loss:0.012589849075139098\n",
      "train loss:0.019362329676314353\n",
      "train loss:0.008016768840732246\n",
      "train loss:0.04442341754831189\n",
      "train loss:0.028728646728708052\n",
      "train loss:0.006118687271462468\n",
      "train loss:0.019695773732198007\n",
      "train loss:0.015403974037195543\n",
      "train loss:0.012243668657010109\n",
      "train loss:0.03926280638555712\n",
      "train loss:0.020167122371980005\n",
      "train loss:0.02009028721982266\n",
      "train loss:0.01811107819678715\n",
      "train loss:0.024111913806506003\n",
      "train loss:0.0086289919264019\n",
      "train loss:0.019001765187252596\n",
      "train loss:0.01758891436592318\n",
      "train loss:0.040452581295248084\n",
      "train loss:0.015262683181039842\n",
      "train loss:0.02948792014288082\n",
      "train loss:0.036005072946205814\n",
      "train loss:0.0178495661420407\n",
      "train loss:0.03970905373005136\n",
      "train loss:0.02125863896189639\n",
      "train loss:0.008120172930792062\n",
      "train loss:0.020276044320357046\n",
      "train loss:0.02512116687796357\n",
      "train loss:0.03128831837608233\n",
      "train loss:0.022959514996212214\n",
      "train loss:0.012653402510608355\n",
      "train loss:0.02625044230393411\n",
      "train loss:0.01622127067283186\n",
      "train loss:0.04866001072066746\n",
      "train loss:0.011988020746504844\n",
      "train loss:0.03036644774156235\n",
      "train loss:0.023342025714431815\n",
      "train loss:0.025483159807876493\n",
      "train loss:0.0555469728817479\n",
      "train loss:0.02331715726993912\n",
      "train loss:0.02476911639293635\n",
      "train loss:0.02669027774030081\n",
      "train loss:0.030571248634425566\n",
      "train loss:0.01774859440417572\n",
      "=== epoch:19, train acc:0.99, test acc:0.956 ===\n",
      "train loss:0.019297703610626018\n",
      "train loss:0.013744356641280557\n",
      "train loss:0.02237330934969572\n",
      "train loss:0.05996052421448507\n",
      "train loss:0.02665184126962402\n",
      "train loss:0.04310335837032613\n",
      "train loss:0.04213012176503056\n",
      "train loss:0.010762863313641678\n",
      "train loss:0.015487071004696828\n",
      "train loss:0.030290038432784577\n",
      "train loss:0.02853832522359421\n",
      "train loss:0.015917739122262225\n",
      "train loss:0.007363047024795496\n",
      "train loss:0.043506922713169215\n",
      "train loss:0.023733102224872606\n",
      "train loss:0.012624279235688684\n",
      "train loss:0.05107672829689367\n",
      "train loss:0.015520199236445797\n",
      "train loss:0.014756288126658294\n",
      "train loss:0.008200227983758147\n",
      "train loss:0.013978864379471075\n",
      "train loss:0.01629276464737744\n",
      "train loss:0.02580644028734054\n",
      "train loss:0.01901129853071099\n",
      "train loss:0.023657366259703078\n",
      "train loss:0.021143253430222293\n",
      "train loss:0.009403150217996549\n",
      "train loss:0.03403823610545556\n",
      "train loss:0.019795701099930348\n",
      "train loss:0.011244199121979055\n",
      "train loss:0.01442174650639297\n",
      "train loss:0.006465319010348415\n",
      "train loss:0.025386382369827418\n",
      "train loss:0.006726790833796384\n",
      "train loss:0.005339123931862274\n",
      "train loss:0.01720768960655337\n",
      "train loss:0.014922592629498785\n",
      "train loss:0.008343571451119174\n",
      "train loss:0.041421722681489366\n",
      "train loss:0.01151892584693083\n",
      "train loss:0.012992322089267223\n",
      "train loss:0.020175719563261908\n",
      "train loss:0.023827899825869386\n",
      "train loss:0.013372595486469605\n",
      "train loss:0.010459395809766996\n",
      "train loss:0.02670232589455921\n",
      "train loss:0.041853856970069374\n",
      "train loss:0.024660026230887682\n",
      "train loss:0.01228354012839765\n",
      "train loss:0.00936358072915758\n",
      "=== epoch:20, train acc:0.992, test acc:0.964 ===\n",
      "train loss:0.009309510528632227\n",
      "train loss:0.02260930846944227\n",
      "train loss:0.03238242759942426\n",
      "train loss:0.01318910749704527\n",
      "train loss:0.02134172497261462\n",
      "train loss:0.017776961463864968\n",
      "train loss:0.06185819068616154\n",
      "train loss:0.023389401483473234\n",
      "train loss:0.012524473726124334\n",
      "train loss:0.010952485159604543\n",
      "train loss:0.01365200023591428\n",
      "train loss:0.01941288014947003\n",
      "train loss:0.0041872152088917106\n",
      "train loss:0.019248009241916025\n",
      "train loss:0.013616667235400039\n",
      "train loss:0.016880194248222413\n",
      "train loss:0.047078180722420655\n",
      "train loss:0.010668348813338313\n",
      "train loss:0.03082571855617379\n",
      "train loss:0.02622229865238143\n",
      "train loss:0.012644227354354085\n",
      "train loss:0.011649573732092144\n",
      "train loss:0.016276332792562128\n",
      "train loss:0.016464947443519035\n",
      "train loss:0.02507004413627427\n",
      "train loss:0.023996318253301334\n",
      "train loss:0.019124361377591964\n",
      "train loss:0.016431574550286855\n",
      "train loss:0.01778812762008195\n",
      "train loss:0.010661382508033217\n",
      "train loss:0.023783327497515924\n",
      "train loss:0.019557570897073228\n",
      "train loss:0.01945463444776316\n",
      "train loss:0.009534936104936933\n",
      "train loss:0.010689438315613031\n",
      "train loss:0.01728833334454822\n",
      "train loss:0.013262658223595625\n",
      "train loss:0.007598214202705768\n",
      "train loss:0.027514977428207677\n",
      "train loss:0.03491892544178209\n",
      "train loss:0.02397549606514501\n",
      "train loss:0.020744467947174817\n",
      "train loss:0.00696745047155548\n",
      "train loss:0.03203948755593826\n",
      "train loss:0.006965615315365171\n",
      "train loss:0.014968072640652015\n",
      "train loss:0.007893012155306462\n",
      "train loss:0.03419997597545693\n",
      "train loss:0.0405967387346697\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.96\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117de3208>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-4.1.1]",
   "language": "python",
   "name": "conda-env-anaconda3-4.1.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipythonP",
  "version": "Python 3.5.2 :: Anaconda custom (x86_64)"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
