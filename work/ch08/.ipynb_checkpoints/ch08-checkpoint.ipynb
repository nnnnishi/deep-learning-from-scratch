{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 ディープラーニング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ネットワークをより深く \n",
    "1. ディープラーニングの小歴史\n",
    "1. \bディープラーニングの高速化\n",
    "1. ディープラーニングの実用例\n",
    "1. ディープラーニングの未来\n",
    "1. まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGを参考にしたネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "\n",
    "\n",
    "class DeepConvNet:\n",
    "    \"\"\"認識率99%以上の高精度なConvNet\n",
    "\n",
    "    ネットワーク構成は下記の通り\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 重みの初期化===========\n",
    "        # 各層のニューロンひとつあたりが、前層のニューロンといくつのつながりがあるか（TODO:自動で計算する）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        weight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLUを使う場合に推奨される初期値\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = weight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = weight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = weight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.271367856874713\n",
      "=== epoch:1, train acc:0.138, test acc:0.138 ===\n",
      "train loss:2.3093065798117673\n",
      "train loss:2.3338022173781\n",
      "train loss:2.283430268481283\n",
      "train loss:2.31293705894189\n",
      "train loss:2.311641985053871\n",
      "train loss:2.2953986692649213\n",
      "train loss:2.2785976169684363\n",
      "train loss:2.2558599920139466\n",
      "train loss:2.26710458333315\n",
      "train loss:2.253378483407437\n",
      "train loss:2.194322934215733\n",
      "train loss:2.2266169161647853\n",
      "train loss:2.214622217369581\n",
      "train loss:2.2094290233538816\n",
      "train loss:2.222369595648069\n",
      "train loss:2.2224286997763096\n",
      "train loss:2.1968679455668205\n",
      "train loss:2.1590577908898196\n",
      "train loss:2.192396066151759\n",
      "train loss:2.180343182685085\n",
      "train loss:2.1583027157188424\n",
      "train loss:2.054341488640579\n",
      "train loss:2.1181417053290743\n",
      "train loss:2.0068527888969037\n",
      "train loss:2.055764672159217\n",
      "train loss:2.0560819446841863\n",
      "train loss:2.0114480413224816\n",
      "train loss:1.9267946046068525\n",
      "train loss:1.9823872059056442\n",
      "train loss:1.8989418297020206\n",
      "train loss:1.9917873412497105\n",
      "train loss:2.087788776803016\n",
      "train loss:2.024909542117117\n",
      "train loss:1.8770100531451763\n",
      "train loss:1.9621448028087263\n",
      "train loss:1.9472507570357704\n",
      "train loss:1.9465613306157874\n",
      "train loss:2.067613008704326\n",
      "train loss:1.9552340143854121\n",
      "train loss:1.8166954550050445\n",
      "train loss:1.713086545753753\n",
      "train loss:1.9428935744296318\n",
      "train loss:1.9657860180698135\n",
      "train loss:1.7490369466183358\n",
      "train loss:1.8008011646539268\n",
      "train loss:1.9871310214484148\n",
      "train loss:1.7318249140631645\n",
      "train loss:1.9419178974085716\n",
      "train loss:1.8292246865020851\n",
      "train loss:1.8325539973728673\n",
      "=== epoch:2, train acc:0.83, test acc:0.8 ===\n",
      "train loss:1.7715852212310332\n",
      "train loss:1.8549861518926465\n",
      "train loss:1.7809431651063603\n",
      "train loss:1.6447993960200284\n",
      "train loss:1.7376051532502614\n",
      "train loss:1.9052174455039366\n",
      "train loss:1.8219397109446205\n",
      "train loss:1.6387240921011388\n",
      "train loss:1.7186708108007591\n",
      "train loss:1.7058650341196049\n",
      "train loss:1.8121536809869816\n",
      "train loss:1.5898406751456498\n",
      "train loss:1.5283105949466111\n",
      "train loss:1.6109855255896073\n",
      "train loss:1.755858310200233\n",
      "train loss:1.6427530830506598\n",
      "train loss:1.6790824676776261\n",
      "train loss:1.6679133245656133\n",
      "train loss:1.6883446162228881\n",
      "train loss:1.649312348262583\n",
      "train loss:1.642757000307207\n",
      "train loss:1.5145477577277007\n",
      "train loss:1.631899013255461\n",
      "train loss:1.8977117513139827\n",
      "train loss:1.496480582524175\n",
      "train loss:1.5769342738358594\n",
      "train loss:1.610708517220925\n",
      "train loss:1.6649685395766656\n",
      "train loss:1.6507451276704557\n",
      "train loss:1.5805230241950312\n",
      "train loss:1.5539515488275455\n",
      "train loss:1.5622150487402646\n",
      "train loss:1.6485886523268796\n",
      "train loss:1.69606226703591\n",
      "train loss:1.6207839613637447\n",
      "train loss:1.3305813448802237\n",
      "train loss:1.5617731945827062\n",
      "train loss:1.5565608526043757\n",
      "train loss:1.608648206420478\n",
      "train loss:1.471310905449325\n",
      "train loss:1.5890441644797548\n",
      "train loss:1.7151494471158057\n",
      "train loss:1.4188821530210571\n",
      "train loss:1.5586359050482432\n",
      "train loss:1.3704463363608503\n",
      "train loss:1.6538389689359587\n",
      "train loss:1.6976191541236723\n",
      "train loss:1.4558912690459866\n",
      "train loss:1.4381722975531879\n",
      "train loss:1.4688303246469108\n",
      "=== epoch:3, train acc:0.907, test acc:0.877 ===\n",
      "train loss:1.5723902884958378\n",
      "train loss:1.6047424810532869\n",
      "train loss:1.5483139730444395\n",
      "train loss:1.5697311012066373\n",
      "train loss:1.7262313635182875\n",
      "train loss:1.7089850384871699\n",
      "train loss:1.4546515010512868\n",
      "train loss:1.5221184507855738\n",
      "train loss:1.4026602965490012\n",
      "train loss:1.33879796398598\n",
      "train loss:1.3777208566584982\n",
      "train loss:1.592349541758472\n",
      "train loss:1.6160444676634944\n",
      "train loss:1.5329432105531005\n",
      "train loss:1.5528745130264507\n",
      "train loss:1.5618519078437298\n",
      "train loss:1.4761941310679612\n",
      "train loss:1.4395800510874426\n",
      "train loss:1.379754726261075\n",
      "train loss:1.6189063331235491\n",
      "train loss:1.6415467447726813\n",
      "train loss:1.6563697335035974\n",
      "train loss:1.4145371244819949\n",
      "train loss:1.4924212953947438\n",
      "train loss:1.5067603610898845\n",
      "train loss:1.6199625215063973\n",
      "train loss:1.6067821229303427\n",
      "train loss:1.6114753780225533\n",
      "train loss:1.3907864094007492\n",
      "train loss:1.3772787878533956\n",
      "train loss:1.3350887425899465\n",
      "train loss:1.4659640098874058\n",
      "train loss:1.5351641887277663\n",
      "train loss:1.399388622891079\n",
      "train loss:1.4944644358775847\n",
      "train loss:1.2831355038787422\n",
      "train loss:1.2722592806219628\n",
      "train loss:1.4038202404942048\n",
      "train loss:1.3310198971001572\n",
      "train loss:1.3075388865674182\n",
      "train loss:1.467349982783481\n",
      "train loss:1.559478554561951\n",
      "train loss:1.5198005582359402\n",
      "train loss:1.490996536962964\n",
      "train loss:1.3079419658118943\n",
      "train loss:1.449354906075185\n",
      "train loss:1.4133782549636738\n",
      "train loss:1.431862337306174\n",
      "train loss:1.5548475862084497\n",
      "train loss:1.4673065889766863\n",
      "=== epoch:4, train acc:0.945, test acc:0.923 ===\n",
      "train loss:1.3964035981150886\n",
      "train loss:1.3991808277704956\n",
      "train loss:1.2601305259287228\n",
      "train loss:1.5218505321156712\n",
      "train loss:1.3966290626640896\n",
      "train loss:1.5564162874760108\n",
      "train loss:1.3887920820823212\n",
      "train loss:1.4759069614872717\n",
      "train loss:1.4247820753214437\n",
      "train loss:1.3370847409380213\n",
      "train loss:1.3779267619207645\n",
      "train loss:1.5006074755020413\n",
      "train loss:1.3017649455653455\n",
      "train loss:1.3300026104488034\n",
      "train loss:1.4249150783340796\n",
      "train loss:1.3089678942890608\n",
      "train loss:1.2702464727100922\n",
      "train loss:1.1620085760999799\n",
      "train loss:1.4007849019348197\n",
      "train loss:1.3480977959088236\n",
      "train loss:1.2941323907767242\n",
      "train loss:1.2636982556863152\n",
      "train loss:1.4024357440451418\n",
      "train loss:1.5443194234064643\n",
      "train loss:1.2010731825877257\n",
      "train loss:1.3824295496149628\n",
      "train loss:1.4457113784967979\n",
      "train loss:1.2878362499870837\n",
      "train loss:1.4541271997737024\n",
      "train loss:1.3955360189780148\n",
      "train loss:1.2798725897752852\n",
      "train loss:1.3368176537289256\n",
      "train loss:1.1719560109629605\n",
      "train loss:1.3763195783623285\n",
      "train loss:1.2227786444238473\n",
      "train loss:1.2050027574127804\n",
      "train loss:1.1554567054583693\n",
      "train loss:1.379496844141056\n",
      "train loss:1.4250489907109463\n",
      "train loss:1.169283377159028\n",
      "train loss:1.3289095343902926\n",
      "train loss:1.2987413603069229\n",
      "train loss:1.4270968313707555\n",
      "train loss:1.2936269312871027\n",
      "train loss:1.247317120765116\n",
      "train loss:1.286924583762499\n",
      "train loss:1.3968846067177116\n",
      "train loss:1.32824731239339\n",
      "train loss:1.3690234665943344\n",
      "train loss:1.393567954512594\n",
      "=== epoch:5, train acc:0.961, test acc:0.947 ===\n",
      "train loss:1.1716035759918748\n",
      "train loss:1.209783281996179\n",
      "train loss:1.424070246360823\n",
      "train loss:1.4352033751995372\n",
      "train loss:1.196280803419533\n",
      "train loss:1.4299345501763852\n",
      "train loss:1.3879973684755385\n",
      "train loss:1.3361110200143165\n",
      "train loss:1.270221329103625\n",
      "train loss:1.3080192634636498\n",
      "train loss:1.2699377315460734\n",
      "train loss:1.1812792247962012\n",
      "train loss:1.1130039175714515\n",
      "train loss:1.3192639274495142\n",
      "train loss:1.3879452371864434\n",
      "train loss:1.1934784094862567\n",
      "train loss:1.0791418969805024\n",
      "train loss:1.1342640342882042\n",
      "train loss:1.2992728913304794\n",
      "train loss:1.4279241361142523\n",
      "train loss:1.2286900806588532\n",
      "train loss:1.360368908953002\n",
      "train loss:1.190492195947555\n",
      "train loss:1.0916691701050225\n",
      "train loss:1.1262511388587533\n",
      "train loss:1.294763751040763\n",
      "train loss:1.2323434006762581\n",
      "train loss:1.4045351716217658\n",
      "train loss:1.151926261540222\n",
      "train loss:1.3496956236530897\n",
      "train loss:1.2516102577262225\n",
      "train loss:1.237606343893498\n",
      "train loss:1.2647010037403326\n",
      "train loss:1.3826728807999276\n",
      "train loss:1.3018643835752457\n",
      "train loss:1.2482085115016937\n",
      "train loss:1.2525325095967115\n",
      "train loss:1.172199519337276\n",
      "train loss:1.3743365644752288\n",
      "train loss:1.2139394400764465\n",
      "train loss:1.2403982652830732\n",
      "train loss:1.1716378896307666\n",
      "train loss:1.2958722914820024\n",
      "train loss:1.1566608833991847\n",
      "train loss:1.3432433544352476\n",
      "train loss:1.2578635581882334\n",
      "train loss:1.3393489249036168\n",
      "train loss:1.3036792219592344\n",
      "train loss:1.151944828709758\n",
      "train loss:1.1298808458569587\n",
      "=== epoch:6, train acc:0.964, test acc:0.95 ===\n",
      "train loss:1.119989945649297\n",
      "train loss:1.2556495084215662\n",
      "train loss:1.2590410709177795\n",
      "train loss:1.0782877761244678\n",
      "train loss:1.279220561494353\n",
      "train loss:1.524104434253905\n",
      "train loss:1.1981021215312913\n",
      "train loss:1.2538200592873001\n",
      "train loss:1.2411254758043353\n",
      "train loss:1.1621998325378569\n",
      "train loss:1.3224996703667447\n",
      "train loss:1.240086893754896\n",
      "train loss:1.1634017228446967\n",
      "train loss:1.1703454872663175\n",
      "train loss:1.1898193550275518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.1962028111133203\n",
      "train loss:1.2082887514434055\n",
      "train loss:1.0231303282303439\n",
      "train loss:1.1178255058407145\n",
      "train loss:1.206204603757602\n",
      "train loss:1.0105357368185022\n",
      "train loss:1.111255297583602\n",
      "train loss:1.086598196765533\n",
      "train loss:1.2055959425852691\n",
      "train loss:1.209925194272414\n",
      "train loss:1.131127891907511\n",
      "train loss:1.0146295439036808\n",
      "train loss:1.0939846399077733\n",
      "train loss:1.2046211777960125\n",
      "train loss:1.0508594105433577\n",
      "train loss:1.0906910878291534\n",
      "train loss:1.1738396445309314\n",
      "train loss:1.2797642221192866\n",
      "train loss:1.1647326797236446\n",
      "train loss:1.2139914157692426\n",
      "train loss:1.2194700510879035\n",
      "train loss:1.308043017743179\n",
      "train loss:1.1855723868964423\n",
      "train loss:1.2789818800456105\n",
      "train loss:1.1825254677516903\n",
      "train loss:0.960599713957729\n",
      "train loss:1.17448623727598\n",
      "train loss:1.0629601444406098\n",
      "train loss:1.085253099057898\n",
      "train loss:1.0532919460560253\n",
      "train loss:1.3019654582814968\n",
      "train loss:1.2437927787410943\n",
      "train loss:1.1301199356656892\n",
      "train loss:1.1461687520717754\n",
      "train loss:1.2382126852950444\n",
      "=== epoch:7, train acc:0.967, test acc:0.964 ===\n",
      "train loss:1.2306335057016569\n",
      "train loss:1.1224130175329095\n",
      "train loss:1.1592693917173589\n",
      "train loss:1.364121589120919\n",
      "train loss:1.158918696081019\n",
      "train loss:1.2074076124309903\n",
      "train loss:1.193968567603865\n",
      "train loss:1.0543423004196015\n",
      "train loss:1.3141478235109973\n",
      "train loss:1.163633636425565\n",
      "train loss:1.2126100365689094\n",
      "train loss:1.2929223424266467\n",
      "train loss:1.183374528655908\n",
      "train loss:1.2427903772438587\n",
      "train loss:1.1649350491020356\n",
      "train loss:1.1054394560311607\n",
      "train loss:1.083625416446273\n",
      "train loss:1.0765685137805856\n",
      "train loss:1.0756199868437242\n",
      "train loss:1.0701470334897245\n",
      "train loss:1.2941040736554137\n",
      "train loss:1.2860728673997897\n",
      "train loss:1.0996598425096016\n",
      "train loss:1.179184408238677\n",
      "train loss:1.2207548865102162\n",
      "train loss:1.280516929528579\n",
      "train loss:1.0523944936035652\n",
      "train loss:1.2357271169760051\n",
      "train loss:1.1180240451712804\n",
      "train loss:1.2291640579882597\n",
      "train loss:1.0726296690946246\n",
      "train loss:1.0967834144756756\n",
      "train loss:1.229667591390968\n",
      "train loss:1.1077643135390733\n",
      "train loss:1.013147651385564\n",
      "train loss:1.2464088785086433\n",
      "train loss:1.0351391306858668\n",
      "train loss:1.1782637130827043\n",
      "train loss:1.1843023341481989\n",
      "train loss:1.1424720686255518\n",
      "train loss:1.1198427946557934\n",
      "train loss:1.1620052638246179\n",
      "train loss:1.0605495192017564\n",
      "train loss:1.179346125250932\n",
      "train loss:1.167432638142133\n",
      "train loss:1.1633834532075829\n",
      "train loss:1.1223625410422997\n",
      "train loss:1.201655384144608\n",
      "train loss:1.2146402294067722\n",
      "train loss:1.2196589127634687\n",
      "=== epoch:8, train acc:0.97, test acc:0.957 ===\n",
      "train loss:1.139819683426006\n",
      "train loss:1.2317574618194558\n",
      "train loss:1.0551183626724314\n",
      "train loss:1.2445707800719241\n",
      "train loss:1.215354064284434\n",
      "train loss:1.2956110598817951\n",
      "train loss:0.968785803128796\n",
      "train loss:1.1105364569252296\n",
      "train loss:0.9517949730476292\n",
      "train loss:1.172910563526339\n",
      "train loss:1.0995216882725776\n",
      "train loss:1.0781322989493214\n",
      "train loss:1.2497682397450454\n",
      "train loss:1.3828798649832919\n",
      "train loss:1.1023534889469355\n",
      "train loss:1.2547935986000451\n",
      "train loss:1.1147392749470173\n",
      "train loss:1.0127067885002656\n",
      "train loss:1.1431800533025305\n",
      "train loss:1.0615007417649627\n",
      "train loss:1.0839524650431216\n",
      "train loss:0.9996821891563719\n",
      "train loss:1.0965605032574397\n",
      "train loss:1.0387796276498713\n",
      "train loss:1.196162978963564\n",
      "train loss:0.9070091386742969\n",
      "train loss:1.1266665542082226\n",
      "train loss:0.9900812462139867\n",
      "train loss:1.1109311044597887\n",
      "train loss:1.1528968263326176\n",
      "train loss:1.0424754345617857\n",
      "train loss:1.2785237239897813\n",
      "train loss:1.2587801460683743\n",
      "train loss:1.2802496158520742\n",
      "train loss:1.0185026270978037\n",
      "train loss:1.102748948227964\n",
      "train loss:1.1104702883627793\n",
      "train loss:1.1062231111731438\n",
      "train loss:1.0831358827749726\n",
      "train loss:1.1599685426841178\n",
      "train loss:0.9455786922967883\n",
      "train loss:1.0169426732793567\n",
      "train loss:1.16028433989953\n",
      "train loss:1.1261578815153168\n",
      "train loss:1.0815939501371248\n",
      "train loss:0.9434087824968802\n",
      "train loss:1.002478429874221\n",
      "train loss:1.0690582407204514\n",
      "train loss:1.1509506581646758\n",
      "train loss:1.228346818215774\n",
      "=== epoch:9, train acc:0.975, test acc:0.96 ===\n",
      "train loss:0.8345338266859663\n",
      "train loss:1.1271046207299167\n",
      "train loss:1.135792519981156\n",
      "train loss:1.203464182812801\n",
      "train loss:0.8974828128428911\n",
      "train loss:1.3221561029111388\n",
      "train loss:1.1289425415722394\n",
      "train loss:1.116720458175408\n",
      "train loss:1.093158927608852\n",
      "train loss:1.0692269074564493\n",
      "train loss:1.0337991767846382\n",
      "train loss:1.168270240116572\n",
      "train loss:1.121325585945484\n",
      "train loss:1.1701582226432437\n",
      "train loss:1.1605379264806945\n",
      "train loss:1.1372120789228624\n",
      "train loss:1.1018880629731347\n",
      "train loss:1.051704344669668\n",
      "train loss:1.0163616375756372\n",
      "train loss:1.126796953857166\n",
      "train loss:1.078652632824448\n",
      "train loss:1.2034378269300823\n",
      "train loss:0.9671634019808502\n",
      "train loss:0.9759303815394615\n",
      "train loss:1.0609657911585557\n",
      "train loss:1.1467845699783379\n",
      "train loss:1.0391264641687923\n",
      "train loss:0.9011528748354248\n",
      "train loss:1.1187842772844672\n",
      "train loss:1.019862252939892\n",
      "train loss:1.023226384788949\n",
      "train loss:0.9026709879535503\n",
      "train loss:0.9768618479278733\n",
      "train loss:1.2339851122123873\n",
      "train loss:1.120112754845638\n",
      "train loss:1.3238882394409723\n",
      "train loss:1.2115755605115703\n",
      "train loss:0.9165993276001779\n",
      "train loss:1.1228860144402626\n",
      "train loss:1.0946921347966645\n",
      "train loss:1.2095400702788601\n",
      "train loss:1.2411356436648824\n",
      "train loss:0.9962650600971298\n",
      "train loss:1.2321212423384542\n",
      "train loss:1.0272514087438664\n",
      "train loss:0.9174332366896648\n",
      "train loss:1.0610806296676936\n",
      "train loss:1.1279620576447755\n",
      "train loss:1.0530513629431952\n",
      "train loss:1.115957100596028\n",
      "=== epoch:10, train acc:0.978, test acc:0.969 ===\n",
      "train loss:1.0652941373247196\n",
      "train loss:1.0183812038907998\n",
      "train loss:1.137730824737367\n",
      "train loss:1.1153518849463833\n",
      "train loss:1.1031686941376415\n",
      "train loss:1.0234843423695892\n",
      "train loss:1.0242576866141437\n",
      "train loss:1.0549865850876936\n",
      "train loss:1.0417726872947484\n",
      "train loss:1.0963138276586022\n",
      "train loss:1.139794296688393\n",
      "train loss:1.0155656501343673\n",
      "train loss:1.1439474292621772\n",
      "train loss:1.036122634115823\n",
      "train loss:0.96621302384548\n",
      "train loss:1.1803404844760617\n",
      "train loss:1.1226197943217096\n",
      "train loss:1.0161646923295524\n",
      "train loss:1.1083048399820117\n",
      "train loss:1.0234985659447424\n",
      "train loss:0.9413465113227789\n",
      "train loss:1.180483336166241\n",
      "train loss:1.0733997218870561\n",
      "train loss:0.9742398711130786\n",
      "train loss:1.2538377447559308\n",
      "train loss:1.1942183492630116\n",
      "train loss:1.1035684965994208\n",
      "train loss:1.106220159279933\n",
      "train loss:1.1040186370757614\n",
      "train loss:1.117513640228828\n",
      "train loss:1.0178356372344926\n",
      "train loss:1.0695793897156742\n",
      "train loss:1.0955496810513365\n",
      "train loss:1.195690015668601\n",
      "train loss:1.0979619973454322\n",
      "train loss:0.9628795384664299\n",
      "train loss:0.9898837549748779\n",
      "train loss:1.1027752452177348\n",
      "train loss:0.960332706628865\n",
      "train loss:1.0881230962059427\n",
      "train loss:1.125160757318644\n",
      "train loss:0.9561395466938235\n",
      "train loss:0.9313511217762457\n",
      "train loss:1.0506597438462117\n",
      "train loss:1.039156908083691\n",
      "train loss:1.1021632813371864\n",
      "train loss:1.0886003841321468\n",
      "train loss:1.094751457810793\n",
      "train loss:1.1268814582331084\n",
      "train loss:1.0637877940766898\n",
      "=== epoch:11, train acc:0.977, test acc:0.957 ===\n",
      "train loss:0.9481855007859137\n",
      "train loss:1.1705897535814964\n",
      "train loss:1.2653476154824617\n",
      "train loss:1.1307547718437758\n",
      "train loss:1.1782780150728984\n",
      "train loss:1.1291490874737253\n",
      "train loss:1.138767732959681\n",
      "train loss:1.1101427514865554\n",
      "train loss:0.9274846807732752\n",
      "train loss:0.919645183026719\n",
      "train loss:0.8391972872728263\n",
      "train loss:1.3388045989811002\n",
      "train loss:1.0452374554241164\n",
      "train loss:1.1021783418463262\n",
      "train loss:1.0865656122884477\n",
      "train loss:1.1074633477071838\n",
      "train loss:1.100453019622564\n",
      "train loss:1.0607948430268037\n",
      "train loss:1.1036101415904231\n",
      "train loss:1.059969240314614\n",
      "train loss:1.131011122993092\n",
      "train loss:1.0257468170460267\n",
      "train loss:1.3575817360213087\n",
      "train loss:1.1432696893195156\n",
      "train loss:1.1512361005385514\n",
      "train loss:1.1083275895921936\n",
      "train loss:1.043544596150245\n",
      "train loss:1.1761314359370163\n",
      "train loss:1.1456949841710269\n",
      "train loss:1.0647725981442036\n",
      "train loss:1.08641923183481\n",
      "train loss:1.0077354687868634\n",
      "train loss:1.1236383320196637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.1774594904852886\n",
      "train loss:1.267784361770809\n",
      "train loss:1.086175967155092\n",
      "train loss:0.9966782689783973\n",
      "train loss:1.12542432405072\n",
      "train loss:0.8830526307939535\n",
      "train loss:0.9274860600542695\n",
      "train loss:0.9880271675957694\n",
      "train loss:1.2704651172944394\n",
      "train loss:1.003196750367258\n",
      "train loss:0.9249201140512766\n",
      "train loss:1.1238040115537435\n",
      "train loss:1.0322420077126455\n",
      "train loss:1.1613003936088235\n",
      "train loss:0.9889148642418784\n",
      "train loss:1.031268917937514\n",
      "train loss:0.8727126583490279\n",
      "=== epoch:12, train acc:0.985, test acc:0.971 ===\n",
      "train loss:1.1172287908066287\n",
      "train loss:1.1069697369634617\n",
      "train loss:1.0015198987380116\n",
      "train loss:1.3401219680916807\n",
      "train loss:1.0265282554454032\n",
      "train loss:0.999645727557449\n",
      "train loss:1.2436292651992427\n",
      "train loss:1.0478684334257207\n",
      "train loss:0.9235326839955889\n",
      "train loss:0.9380471822284453\n",
      "train loss:1.0673141148953167\n",
      "train loss:0.9663730333647397\n",
      "train loss:1.0660424181871928\n",
      "train loss:1.188237597929562\n",
      "train loss:1.078111382322742\n",
      "train loss:0.9365622899125583\n",
      "train loss:1.0684413609738956\n",
      "train loss:1.019121154334973\n",
      "train loss:1.214017462204894\n",
      "train loss:1.009670887047656\n",
      "train loss:1.1331333590370398\n",
      "train loss:1.033521961475906\n",
      "train loss:1.068967745656744\n",
      "train loss:1.077762212603944\n",
      "train loss:1.1291240893059116\n",
      "train loss:1.0102812959082486\n",
      "train loss:0.924948919716046\n",
      "train loss:0.9171818923720827\n",
      "train loss:0.9716826034430716\n",
      "train loss:0.9883201716773605\n",
      "train loss:0.9783993456070081\n",
      "train loss:1.1389449185332918\n",
      "train loss:1.1359341304685477\n",
      "train loss:1.184626372269324\n",
      "train loss:0.9298564017448429\n",
      "train loss:1.0821638595193708\n",
      "train loss:1.1328051680382638\n",
      "train loss:1.0548545266618146\n",
      "train loss:1.0900476976477085\n",
      "train loss:1.065535541070464\n",
      "train loss:1.1756422159969282\n",
      "train loss:0.9962606232074617\n",
      "train loss:1.1140366137895257\n",
      "train loss:1.057392191720533\n",
      "train loss:1.1352632362490744\n",
      "train loss:1.1267315635252517\n",
      "train loss:1.1607200398339317\n",
      "train loss:0.9235544981381044\n",
      "train loss:1.0747565505221874\n",
      "train loss:0.9142182170646145\n",
      "=== epoch:13, train acc:0.986, test acc:0.969 ===\n",
      "train loss:1.1187097724308752\n",
      "train loss:1.208802489128353\n",
      "train loss:0.9372298978535737\n",
      "train loss:1.0180673631046202\n",
      "train loss:1.0198527264010742\n",
      "train loss:1.0298792354507618\n",
      "train loss:0.8844271970872457\n",
      "train loss:0.9298879234205103\n",
      "train loss:1.1238895956856647\n",
      "train loss:1.0880273457342313\n",
      "train loss:1.1075343031463107\n",
      "train loss:1.095896137832325\n",
      "train loss:0.8160176366804502\n",
      "train loss:0.9052484240640557\n",
      "train loss:1.1841380068228942\n",
      "train loss:0.9165725059621574\n",
      "train loss:0.9546309391622038\n",
      "train loss:1.0251364499985263\n",
      "train loss:0.9290017552655077\n",
      "train loss:1.065950616278946\n",
      "train loss:1.154854610156106\n",
      "train loss:1.0113257926278896\n",
      "train loss:1.0320089131037353\n",
      "train loss:0.9704147562106735\n",
      "train loss:0.9491321106944188\n",
      "train loss:1.0285530287221771\n",
      "train loss:0.9621382697514779\n",
      "train loss:0.8334865937889957\n",
      "train loss:0.841641286594161\n",
      "train loss:1.0417324149223124\n",
      "train loss:0.9384278127367222\n",
      "train loss:1.0210487285631904\n",
      "train loss:1.0071879301047142\n",
      "train loss:0.9834203147013687\n",
      "train loss:0.994417900582654\n",
      "train loss:0.9713947803773674\n",
      "train loss:1.0686278317959246\n",
      "train loss:1.0852345492815239\n",
      "train loss:0.9137025994538746\n",
      "train loss:0.9604608906346124\n",
      "train loss:0.8812736091271419\n",
      "train loss:0.8207714223919274\n",
      "train loss:0.881860314699018\n",
      "train loss:1.0729762741005755\n",
      "train loss:1.1508070564787594\n",
      "train loss:0.9423575505173255\n",
      "train loss:1.0285179210832707\n",
      "train loss:1.0008201817261428\n",
      "train loss:1.0331046801826647\n",
      "train loss:1.1765962149223226\n",
      "=== epoch:14, train acc:0.982, test acc:0.973 ===\n",
      "train loss:0.8158885318568969\n",
      "train loss:1.165670392706837\n",
      "train loss:1.092287020741662\n",
      "train loss:0.9255627227643677\n",
      "train loss:1.0741776756257946\n",
      "train loss:0.9341980564428356\n",
      "train loss:1.0219964630607046\n",
      "train loss:0.9342052839186304\n",
      "train loss:1.0195028960800394\n",
      "train loss:0.9080872072297653\n",
      "train loss:0.9741844423069973\n",
      "train loss:1.0406469842663744\n",
      "train loss:0.8977064071626437\n",
      "train loss:0.9203429498627367\n",
      "train loss:0.8889442372787774\n",
      "train loss:0.9834678384558134\n",
      "train loss:0.9872816209785928\n",
      "train loss:0.9366993726010022\n",
      "train loss:1.0577039025054304\n",
      "train loss:1.0346503447942126\n",
      "train loss:0.8967599044440834\n",
      "train loss:0.8792614347704724\n",
      "train loss:1.0307733877688599\n",
      "train loss:1.0989255319408178\n",
      "train loss:1.0324501383953069\n",
      "train loss:0.9383815658766563\n",
      "train loss:1.1973904063456355\n",
      "train loss:0.874841024459256\n",
      "train loss:1.0237481311821768\n",
      "train loss:1.255146020034723\n",
      "train loss:1.1638234570127162\n",
      "train loss:0.9563758517505089\n",
      "train loss:0.91660090193819\n",
      "train loss:0.9602304201307974\n",
      "train loss:1.0654336559100712\n",
      "train loss:0.99892496994121\n",
      "train loss:0.9222158075031309\n",
      "train loss:1.0049744243956196\n",
      "train loss:0.9021119465153727\n",
      "train loss:1.128605417190235\n",
      "train loss:1.1066724470076417\n",
      "train loss:0.8580889167654578\n",
      "train loss:0.8730427454458284\n",
      "train loss:0.8938177298425458\n",
      "train loss:0.862359789752273\n",
      "train loss:1.0678258429206857\n",
      "train loss:1.0938688092398774\n",
      "train loss:1.000360547463939\n",
      "train loss:1.0593454577046166\n",
      "train loss:0.86543613784029\n",
      "=== epoch:15, train acc:0.988, test acc:0.977 ===\n",
      "train loss:1.1238548361520553\n",
      "train loss:1.1614800203524889\n",
      "train loss:1.199064118575373\n",
      "train loss:0.9610101807781303\n",
      "train loss:1.0473535045083453\n",
      "train loss:0.9125847113937762\n",
      "train loss:0.9652806568374415\n",
      "train loss:0.9646446481511157\n",
      "train loss:1.2696949809514382\n",
      "train loss:1.0865100821910336\n",
      "train loss:0.9907660144883643\n",
      "train loss:0.8444891117310667\n",
      "train loss:0.8602025809192912\n",
      "train loss:0.9235940731858855\n",
      "train loss:1.1759806349282942\n",
      "train loss:1.0306073508285432\n",
      "train loss:0.942056320983879\n",
      "train loss:1.0071278078095225\n",
      "train loss:0.9432983286730058\n",
      "train loss:0.9468873632832896\n",
      "train loss:1.124536618542859\n",
      "train loss:1.1218424641320652\n",
      "train loss:0.8639708117398659\n",
      "train loss:1.2381108278515522\n",
      "train loss:1.0649007123798648\n",
      "train loss:1.0628212951139964\n",
      "train loss:1.1311364801472577\n",
      "train loss:1.1029043545804726\n",
      "train loss:0.9033933264676685\n",
      "train loss:1.0487583671599516\n",
      "train loss:0.9552194975923761\n",
      "train loss:1.1092140158812394\n",
      "train loss:1.1773674078018541\n",
      "train loss:1.0633328081263245\n",
      "train loss:0.87034950758728\n",
      "train loss:1.0540632182834495\n",
      "train loss:0.9285886159445716\n",
      "train loss:0.9317758141534863\n",
      "train loss:0.9915637347319886\n",
      "train loss:0.9492532286230752\n",
      "train loss:0.8736989101125268\n",
      "train loss:0.9541620783048717\n",
      "train loss:1.037360710907582\n",
      "train loss:1.0360858083134632\n",
      "train loss:1.159527620432013\n",
      "train loss:1.1284948347465278\n",
      "train loss:1.0256375939635953\n",
      "train loss:0.9665932677396423\n",
      "train loss:0.9525169443257957\n",
      "train loss:0.9088326806331051\n",
      "=== epoch:16, train acc:0.985, test acc:0.967 ===\n",
      "train loss:1.0210894190714637\n",
      "train loss:1.0009380822758756\n",
      "train loss:1.3078806840972284\n",
      "train loss:0.9019957786136727\n",
      "train loss:1.062244299251819\n",
      "train loss:0.9685904891254296\n",
      "train loss:1.081880063641759\n",
      "train loss:0.9433112330316515\n",
      "train loss:0.8843160146619315\n",
      "train loss:1.1364072356038697\n",
      "train loss:1.0263511851664329\n",
      "train loss:1.0008714253880766\n",
      "train loss:0.9187059882768858\n",
      "train loss:0.8738623744284376\n",
      "train loss:1.1344402189086145\n",
      "train loss:0.9360940530982361\n",
      "train loss:1.0305583448054925\n",
      "train loss:0.9074701090892341\n",
      "train loss:0.904339468287846\n",
      "train loss:0.8737684629833511\n",
      "train loss:1.0811226296639171\n",
      "train loss:0.9605205509266886\n",
      "train loss:1.1521910544558938\n",
      "train loss:1.0551639526763608\n",
      "train loss:0.8837777148439101\n",
      "train loss:0.9053595951663544\n",
      "train loss:0.8664170524441533\n",
      "train loss:0.9747748653778748\n",
      "train loss:1.0180152183625055\n",
      "train loss:0.9939652927461161\n",
      "train loss:0.9343859377497518\n",
      "train loss:0.931160353404841\n",
      "train loss:0.9897580485644321\n",
      "train loss:1.0270348994593244\n",
      "train loss:0.9132929758821048\n",
      "train loss:0.973957487460835\n",
      "train loss:1.0742556217458803\n",
      "train loss:1.0071681106054626\n",
      "train loss:1.0945026027784077\n",
      "train loss:1.1776958936572826\n",
      "train loss:1.0658565502123596\n",
      "train loss:1.0626181702796609\n",
      "train loss:1.0042372543840878\n",
      "train loss:1.015367209856275\n",
      "train loss:1.0620552456128953\n",
      "train loss:0.8452145385131172\n",
      "train loss:1.0889990709173307\n",
      "train loss:0.9672690142377232\n",
      "train loss:0.9620206438839509\n",
      "train loss:0.9813353676630342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:17, train acc:0.99, test acc:0.978 ===\n",
      "train loss:0.9097990047707062\n",
      "train loss:1.0448983589258451\n",
      "train loss:0.9347306218476358\n",
      "train loss:0.8215188643895407\n",
      "train loss:1.1391815349526289\n",
      "train loss:0.9724090699252437\n",
      "train loss:0.8848494154379739\n",
      "train loss:1.0696060284579434\n",
      "train loss:0.9395408564466607\n",
      "train loss:0.9713355501798858\n",
      "train loss:1.0555513177979083\n",
      "train loss:1.013450681439749\n",
      "train loss:1.2474330683449333\n",
      "train loss:1.0025417382110442\n",
      "train loss:0.8796178067978471\n",
      "train loss:0.824889055400145\n",
      "train loss:1.091780961626756\n",
      "train loss:1.0747759737627371\n",
      "train loss:0.9183426140762482\n",
      "train loss:1.0575291227800725\n",
      "train loss:1.0763155655019385\n",
      "train loss:1.279987175837899\n",
      "train loss:0.7877425836041816\n",
      "train loss:0.9963999154305049\n",
      "train loss:1.123044240986965\n",
      "train loss:1.0617520129263576\n",
      "train loss:0.8866266116189793\n",
      "train loss:0.9963972776766324\n",
      "train loss:0.8784845898176955\n",
      "train loss:1.017961469718903\n",
      "train loss:0.9756658205901438\n",
      "train loss:0.991062898518933\n",
      "train loss:0.9227989273374604\n",
      "train loss:0.9341534328855461\n",
      "train loss:0.9886521055259123\n",
      "train loss:1.0340463735994487\n",
      "train loss:1.081236663897698\n",
      "train loss:0.8955800743407984\n",
      "train loss:0.9888759800751065\n",
      "train loss:0.7888267254619754\n",
      "train loss:0.9036190934990977\n",
      "train loss:0.9707385722163897\n",
      "train loss:0.9281798621278534\n",
      "train loss:1.0668561621120967\n",
      "train loss:0.927018050770482\n",
      "train loss:0.8816962320511239\n",
      "train loss:0.9650415108190112\n",
      "train loss:1.1164321809008373\n",
      "train loss:0.9904249006277414\n",
      "train loss:1.0244877660826786\n",
      "=== epoch:18, train acc:0.994, test acc:0.979 ===\n",
      "train loss:0.7817807391455015\n",
      "train loss:0.9627308038163359\n",
      "train loss:1.0632220387543103\n",
      "train loss:0.9535371028031195\n",
      "train loss:0.9281455303413331\n",
      "train loss:0.9162872595265051\n",
      "train loss:0.9346881259353527\n",
      "train loss:0.8565008682977253\n",
      "train loss:0.9509949039227331\n",
      "train loss:0.9504974563858184\n",
      "train loss:0.9198786319003974\n",
      "train loss:1.0002548639173607\n",
      "train loss:0.8301732932031645\n",
      "train loss:1.0132629990726714\n",
      "train loss:0.8312479541380027\n",
      "train loss:1.0077718196669878\n",
      "train loss:0.9835152219853108\n",
      "train loss:0.8840633815118142\n",
      "train loss:1.0180780789239345\n",
      "train loss:0.9502957030442015\n",
      "train loss:0.9106547185577055\n",
      "train loss:0.978933378074555\n",
      "train loss:0.9001718511936163\n",
      "train loss:0.9617909082045398\n",
      "train loss:0.9716827813234379\n",
      "train loss:0.9324574101038842\n",
      "train loss:0.9758272279674691\n",
      "train loss:0.9501242213111385\n",
      "train loss:1.0176616993139818\n",
      "train loss:0.9420387410459113\n",
      "train loss:1.0068606234219883\n",
      "train loss:0.8905237539184667\n",
      "train loss:0.9564625271858176\n",
      "train loss:1.0152871922011024\n",
      "train loss:0.9753485002409115\n",
      "train loss:1.2240983842328788\n",
      "train loss:0.9478555142719125\n",
      "train loss:1.0838134166059061\n",
      "train loss:0.9702567108059954\n",
      "train loss:0.9907340663370221\n",
      "train loss:0.892270000247583\n",
      "train loss:0.9710714022335799\n",
      "train loss:1.0137051029863757\n",
      "train loss:0.9784115517191967\n",
      "train loss:0.7917904237248008\n",
      "train loss:0.9021301057701602\n",
      "train loss:0.9140396488324565\n",
      "train loss:0.7737073107993936\n",
      "train loss:0.7821987855447544\n",
      "train loss:1.0831140149275165\n",
      "=== epoch:19, train acc:0.99, test acc:0.978 ===\n",
      "train loss:0.9249772704498593\n",
      "train loss:0.9061013536337633\n",
      "train loss:1.1013668422545158\n",
      "train loss:0.963363338913518\n",
      "train loss:1.1576325960375007\n",
      "train loss:1.0974043094912942\n",
      "train loss:0.8476711935235903\n",
      "train loss:0.8122774147537956\n",
      "train loss:0.8837640118198214\n",
      "train loss:0.850629467946101\n",
      "train loss:0.9917554034246548\n",
      "train loss:0.9041162666566409\n",
      "train loss:1.0207232427399073\n",
      "train loss:0.9671043141311082\n",
      "train loss:1.0694617661631955\n",
      "train loss:0.8469380062731975\n",
      "train loss:1.067352738944666\n",
      "train loss:0.8956721425777172\n",
      "train loss:0.9379086518457109\n",
      "train loss:0.8737163987320641\n",
      "train loss:0.9341113352519729\n",
      "train loss:1.1244416970285196\n",
      "train loss:0.8993885751718912\n",
      "train loss:0.9790669591044403\n",
      "train loss:1.1120265910666358\n",
      "train loss:0.9842087209848701\n",
      "train loss:1.0897958439814255\n",
      "train loss:0.8128915594893948\n",
      "train loss:1.0384166887359658\n",
      "train loss:0.8301756948474938\n",
      "train loss:1.0780603336687657\n",
      "train loss:0.8746817570562188\n",
      "train loss:1.2281207702509278\n",
      "train loss:0.9929846908039598\n",
      "train loss:0.9886909758477475\n",
      "train loss:0.9584535159039895\n",
      "train loss:0.858837313370514\n",
      "train loss:0.9904385365127739\n",
      "train loss:0.9893044005701555\n",
      "train loss:1.194523109328465\n",
      "train loss:1.008208037465672\n",
      "train loss:1.169788311276031\n",
      "train loss:0.9692661417469135\n",
      "train loss:0.851791602659827\n",
      "train loss:1.0062136469906007\n",
      "train loss:1.1341426492589028\n",
      "train loss:1.0605420368323006\n",
      "train loss:1.0861028944049032\n",
      "train loss:0.9202898221953016\n",
      "train loss:1.001632374846516\n",
      "=== epoch:20, train acc:0.994, test acc:0.985 ===\n",
      "train loss:1.0557774530807669\n",
      "train loss:0.8030254833490821\n",
      "train loss:0.8709195921638194\n",
      "train loss:0.814684448984991\n",
      "train loss:1.0378984727797012\n",
      "train loss:1.0293388029926756\n",
      "train loss:0.8963097520736771\n",
      "train loss:1.0315090163592107\n",
      "train loss:0.9115465373203919\n",
      "train loss:1.0906934992390587\n",
      "train loss:0.8638917297933116\n",
      "train loss:0.959608520964554\n",
      "train loss:0.9819012082354522\n",
      "train loss:1.0491481079036933\n",
      "train loss:1.045132768851443\n",
      "train loss:0.8006053415876636\n",
      "train loss:1.0796093377655505\n",
      "train loss:0.9546551669731206\n",
      "train loss:1.046584532500302\n",
      "train loss:0.9662883929210011\n",
      "train loss:0.9678210628063354\n",
      "train loss:0.9893850114619288\n",
      "train loss:0.9447325890693249\n",
      "train loss:0.9696765006387481\n",
      "train loss:1.1234545063719634\n",
      "train loss:1.068323167244437\n",
      "train loss:1.2628481683578867\n",
      "train loss:0.8881275507954188\n",
      "train loss:1.022436409677798\n",
      "train loss:1.0549413334371764\n",
      "train loss:0.9888453238483299\n",
      "train loss:1.0588939223817548\n",
      "train loss:1.0683148900811867\n",
      "train loss:0.8895967664208965\n",
      "train loss:0.8225485245732896\n",
      "train loss:0.8186483279937167\n",
      "train loss:1.0473749172591562\n",
      "train loss:1.0727557252896256\n",
      "train loss:0.8553718324080849\n",
      "train loss:0.9340989409764471\n",
      "train loss:0.943249154204955\n",
      "train loss:1.0806816434258204\n",
      "train loss:0.9166367342267199\n",
      "train loss:0.8787897916757994\n",
      "train loss:0.9401966578960629\n",
      "train loss:1.0948962663614448\n",
      "train loss:0.9863896624139789\n",
      "train loss:1.081902004701993\n",
      "train loss:0.9184596975483929\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.979\n",
      "Saved Network Parameters!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-4.1.1]",
   "language": "python",
   "name": "conda-env-anaconda3-4.1.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipythonP",
  "version": "Python 3.5.2 :: Anaconda custom (x86_64)"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
